{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline"
      ],
      "metadata": {
        "id": "euX8FpZvha5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "deadline: 16 декабря 2025, 23:59"
      ],
      "metadata": {
        "id": "Lb39V1sAhw-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В последние годы подход Retrieval-Augmented Generation (RAG) стал одним из ключевых методов повышения качества ответов больших языковых моделей. В этой работе вы должны пошагово реализовать собственную RAG-систему “с нуля”, используя открытые модели и инструменты: HuggingFace, FAISS и классические трансформеры для генерации текста."
      ],
      "metadata": {
        "id": "UTs6rZfHhRPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ПРАВИЛА\n",
        "Домашнее задание выполняется в группе до 4-х человек.\n",
        "Домашнее задание оформляется в виде отчета в jupyter-тетрадке.\n",
        "Отчет должен содержать: имена всех членов группы, нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали.\n",
        "Отчеты, состоящие исключительно из кода, не будут проверены и будут автоматически оценены нулевой оценкой.\n",
        "Плагиат и любое недобросовестное цитирование приводит к обнулению оценки.\n",
        "За каждую неделю просрочки после дедлайна начисляет по 1 штрафному баллу (например, при дедлайне 01.10.25 в 23:59 сдача ДЗ 12.10.25 ведёт к 2 штрафным баллам).\n",
        "Бонусные баллы позволяют повысить общую оценку за ДЗ до максимальной (если были ошибки или недочеты, повлекшие снижение баллов)."
      ],
      "metadata": {
        "id": "9wJundcshlw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gUx8_IH8hMIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f84bce-1447-48e5-d37a-0f5fe306438e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "raw_text = \"\\n\".join(dataset[\"train\"][\"text\"])\n",
        "\n",
        "corpus = raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 1. Подготовка данных: разбиение на чанки [2 балла]\n",
        "\n",
        "Необходимо реализовать функцию `make_chunks(text: str, chunk_size: int, overlap: int) -> List[str]`, которая делит длинный текст на перекрывающиеся фрагменты. Рекомендуемый размер одного чанка — 250–350 символов, а перекрытие — 40–60 символов. Попробуйте несколько комбинаций параметров. После реализации примените функцию к `corpus`, получите список `all_chunks` и выведите общее количество чанков, а также первые два и последние два фрагмента.\n",
        "\n",
        "Затем вычислите для `all_chunks` среднюю, минимальную и максимальную длину чанков. На основе этих значений сделайте вывод, стоит ли изменять `chunk_size` или `overlap`, и достаточно ли полученные фрагменты подходят для смыслового поиска в RAG-системе.\n",
        "\n",
        "Также ответьте в тексте:\n",
        "*зачем нужно перекрытие чанков в RAG и что произойдёт, если сделать `overlap = 0`?*\n"
      ],
      "metadata": {
        "id": "enPP6EsijLT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 3. Создание эмбеддингов [2.5 балла]\n",
        "\n",
        "Теперь нужно закодировать их в векторное представление. Мы будем использовать модель эмбеддингов, например\n",
        "**`sentence-transformers/all-MiniLM-L6-v2`** — она достаточно быстрая и отлично подходит для экспериментов в Colab.\n",
        "\n",
        "Сначала загрузите модель при помощи `sentence-transformers` (или вручную через `transformers` + `AutoModel`). Затем напишите небольшую функцию `embed_text(text: str) -> np.ndarray`. Примените функцию ко всем чанкам из Части 2. В результате у вас должен получиться массив эмбеддингов. Выведите пример эмбеддинга.\n",
        "\n",
        "Чтобы улучшить работу поиска (особенно при cosine similarity), нормализуйте все полученные эмбеддинги до единичной длины и сохраните результат в `embeddings_normalized`.\n",
        "\n",
        "Попробуйте немного проанализировать сами эмбеддинги. Посчитайте среднее расстояние (L2 или cosine) между случайными парами чанков и соседними чанками (i и i+1). Сравните результаты. Насколько соседние чанки действительно ближе друг к другу? Помогает ли перекрытие при разбиении текста сформировать более «плавное» и последовательное векторное пространство?"
      ],
      "metadata": {
        "id": "SMbM9r2fj7rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 4. Построение поискового индекса и реализация поиска [2 балла]\n",
        "\n",
        "Теперь, когда все эмбеддинги получены, пора собрать из них настоящий поисковый индекс. Мы будем использовать FAISS — быстрый и удобный инструмент для поиска ближайших векторов.\n",
        "\n",
        "Сначала подключите FAISS, определите размерность эмбеддингов и создайте индекс. Можно выбрать один из двух вариантов:\n",
        "* **IndexFlatL2**, если хотите искать ближайших соседей по евклидову расстоянию;\n",
        "* **IndexFlatIP**, если используете нормализованные векторы и хотите фактически искать по cosine similarity.\n",
        "\n",
        "Добавьте в него все эмбеддинги (`embeddings_normalized`). Ответьте: сколько векторов добавлено, какая размерность у индекса, чем отличается поиск по L2 от cosine similarity, и почему вы выбрали свой вариант.\n",
        "\n",
        "Далее нужно реализовать простую функцию поиска:\n",
        "\n",
        "```python\n",
        "def search(query: str, k: int = 5):\n",
        "    ...\n",
        "```\n",
        "\n",
        "Функция должна закодировать текст запроса с помощью `embed_text()`, нормализовать вектор, отправить его в FAISS (`index.search`) и вернуть найденные чанки по их индексам.\n",
        "\n",
        "Проверьте, как работает поиск, выполнив запрос по вашему выбору.\n",
        "\n",
        "А затем сформулируйте три собственных вопроса по содержанию корпуса, выполните по каждому поиск top-3 чанков и для каждого результата дайте небольшую оценку релевантности:\n",
        "0 — не похоже,\n",
        "1 — отдалённо связано,\n",
        "2 — действительно по теме."
      ],
      "metadata": {
        "id": "5_poE71ikiSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu-cu12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52_vww0V_6v8",
        "outputId": "a8c8fd1d-6a33-4042-d0ff-7bd3bbc88af6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu-cu12 in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (25.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from faiss-gpu-cu12) (12.6.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 5. Генерация ответов с помощью модели [2.5 балла]\n",
        "\n",
        "Теперь пора собрать полноценный RAG и научиться генерировать ответы.\n",
        "Для этого мы подключим небольшую, бесплатную модель вроде `google/flan-t5-small` (или любую другую модель на ваш выбор).\n",
        "\n",
        "Сначала нужно придумать функцию:\n",
        "\n",
        "```python\n",
        "build_prompt(query: str, context_chunks: List[str]) -> str\n",
        "```\n",
        "\n",
        "Эта функция должна собирать промпт для модели: формулировка вашего вопроса + добавленные кусочки текста, которые модель должна использовать. Стиль промпта — на ваше усмотрение. Можно попробовать несколько вариантов: более строгий, более разговорный, с ограничениями («используй только этот контекст»), и посмотреть, что получится.\n",
        "\n",
        "После этого напишите функцию генерации:\n",
        "\n",
        "```python\n",
        "generate_answer(prompt: str, max_new_tokens: int) -> str\n",
        "```\n",
        "\n",
        "Проверьте её на простом тестовом примере, чтобы убедиться, что всё работает.\n",
        "\n",
        "Теперь можно собрать простой RAG-пайплайн:\n",
        "\n",
        "```python\n",
        "def rag(query: str, k: int = 3) -> str:\n",
        "    found_chunks = search(query, k=k)\n",
        "    prompt = build_prompt(query, found_chunks)\n",
        "    answer = generate_answer(prompt)\n",
        "    return answer\n",
        "```\n",
        "\n",
        "Поиграйте с параметром `k` (например, 1, 3, 5, 7) и посмотрите, как меняется качество ответа.\n",
        "\n",
        "Далее задайте **три своих вопроса** по содержанию корпуса, запустите `rag()` на каждом из них и оцените качество ответа:\n",
        "\n",
        "* **0** — ответ нерелевантен, модель галлюцинирует или игнорирует контекст\n",
        "* **1** — что-то уловила, но не полностью\n",
        "* **2** — чёткий, точный ответ, построенный на найденных чанках\n"
      ],
      "metadata": {
        "id": "vUynD-Oxku2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 6. [1 балл] Итоги\n",
        "Напишите краткое резюме проделанной работы. Что вы сделали на каждом этапе (разбиение текста на чанки, создание эмбеддингов, построение индекса, генерация ответов)? Какие решения сработали хорошо, а что можно улучшить? Как в целом работает собранная вами RAG-система и насколько качественно она отвечает на вопросы?"
      ],
      "metadata": {
        "id": "vTABRQPI1JzM"
      }
    }
  ]
}