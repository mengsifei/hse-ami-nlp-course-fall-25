{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Session 19: The RAG Pipeline - Indexing Knowledge","metadata":{}},{"cell_type":"markdown","source":"**Learning Objectives:**\n\n* Understand why RAG is the primary solution to the LLM knowledge problem.\n* Learn the components of the data ingestion and indexing pipeline using PDF documents.\n* Create and store vector embeddings from custom documents.\n* Perform similarity searches to find relevant information.","metadata":{}},{"cell_type":"markdown","source":"## Part 1: The Theory of Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"### Deep Dive: The Indexing Pipeline\n\nThis is the \"preparation\" phase—making our knowledge searchable for the retrieval step. It's a one-time, upfront process for a given set of documents.\n\n**1. Loading Documents:** The process begins with loading our data. This can come from various sources:\n   * PDFs\n   * Text files (`.txt`, `.md`)\n   * Web pages\n   * Databases\n   * APIs\n   \n**2. Chunking (Splitting):** We can't feed an entire book into an embedding model due to token limits and computational cost. More importantly, retrieving an entire book to answer a specific question is inefficient. We split the documents into small, semantically meaningful chunks. Common strategies include:\n   * **Fixed-size chunking:** Easy but can break sentences apart.\n   * **Recursive character splitting:** A smarter method that tries to split based on semantic boundaries like paragraphs (`\\n\\n`), sentences (`.`), and spaces (` `).\n\n**3. Embedding:** This is where we convert our text chunks into numerical representations (vectors). These vectors capture the semantic meaning of the text. Models called \"embedding models\" (like `all-MiniLM-L6-v2` or OpenAI's `text-embedding-ada-002`) are specifically trained for this task. The key idea is that chunks with similar meanings will have vectors that are close to each other in a high-dimensional space.\n\n**4. Storing (Vector Stores):** A Vector Store, or Vector Database, is a specialized database designed to efficiently store and search through millions or billions of vectors. It's the core component that enables fast similarity searches. \n   * **Popular examples:** Chroma, FAISS, Pinecone, Weaviate.","metadata":{}},{"cell_type":"markdown","source":"### The Power of Semantic Search\n\nThe magic of RAG comes from **semantic search**, which is fundamentally different from traditional **keyword search** (like Ctrl+F).\n\n* **Keyword Search:** Finds exact matches of words. A search for \"car\" will miss documents that only mention \"automobile.\"\n* **Semantic Search:** Finds documents based on meaning. Because the vectors for \"car\" and \"automobile\" are very close in vector space, a semantic search for one will easily find the other. This allows us to find relevant information even if the user's query uses completely different wording than the source document.","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Practical - Building Your Knowledge Base","metadata":{}},{"cell_type":"markdown","source":"### Setup\n\nFirst, let's install the necessary libraries. We're adding `pypdf` to handle the PDF loading.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -q langchain langchain_community langchain_huggingface sentence-transformers faiss-cpu pypdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:20:29.255138Z","iopub.execute_input":"2025-11-07T09:20:29.255404Z","iopub.status.idle":"2025-11-07T09:21:43.552211Z","shell.execute_reply.started":"2025-11-07T09:20:29.255352Z","shell.execute_reply":"2025-11-07T09:21:43.551145Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Code Demo 1: Loading & Chunking a PDF\n\n**Goal:** Load a PDF document from the web and split it into manageable chunks.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install wikipedia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:21:43.553697Z","iopub.execute_input":"2025-11-07T09:21:43.553943Z","iopub.status.idle":"2025-11-07T09:21:48.307561Z","shell.execute_reply.started":"2025-11-07T09:21:43.553918Z","shell.execute_reply":"2025-11-07T09:21:48.306477Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nfrom langchain_community.document_loaders import WikipediaLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nurl = \"https://en.wikipedia.org/wiki/Nefertiti\"\n\nloader = WikipediaLoader(query=\"Nefertiti\", load_max_docs=1)\ndocuments = loader.load()\n\nprint(f\"Loaded {len(documents)} document(s) from {url}\")\n\n# 2. Chunk the document\n# We'll use a slightly larger chunk size for a dense research paper.\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"\\nDocument split into {len(chunks)} chunks.\")\nprint(\"\\n--- Sample Chunk ---\")\nprint(chunks[0].page_content)\nprint(\"------------------------------------------\")\nprint(chunks[1].page_content)\nprint(\"------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:21:48.308731Z","iopub.execute_input":"2025-11-07T09:21:48.308996Z","iopub.status.idle":"2025-11-07T09:21:50.467497Z","shell.execute_reply.started":"2025-11-07T09:21:48.308973Z","shell.execute_reply":"2025-11-07T09:21:50.466752Z"}},"outputs":[{"name":"stdout","text":"Loaded 1 document(s) from https://en.wikipedia.org/wiki/Nefertiti\n\nDocument split into 13 chunks.\n\n--- Sample Chunk ---\nNefertiti () (c. 1370 – c. 1330 BC) was a queen of the 18th Dynasty of Ancient Egypt, the great royal wife of Pharaoh Akhenaten. Nefertiti and her husband were known for their radical overhaul of state religious policy, in which they promoted an exclusivist and possibly even monotheistic religion, Atenism, centered on the sun disc and its direct connection to the royal household. With her husband, she reigned at what was arguably the wealthiest period of ancient Egyptian history.\n------------------------------------------\nAfter her husband's death, some scholars believe that Nefertiti ruled briefly as the female pharaoh known by the throne name Neferneferuaten, and before the ascension of Tutankhamun, although this identification is a matter of ongoing debate. If Nefertiti did rule as pharaoh, her reign was marked by the fall of Amarna and the relocation of the capital back to the traditional city of Thebes.\n------------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Code Demo 2: Creating Embeddings\n\n**Goal:** Convert the text chunks into numerical vectors using an embedding model. (This part remains the same!)","metadata":{}},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\n\n# 3. Initialize the embedding model\n# We use a popular open-source model from Hugging Face\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel_kwargs = {'device': 'cpu'} # Use CPU for this example\nencode_kwargs = {'normalize_embeddings': False}\nembeddings = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n\nprint(\"Embedding model loaded successfully.\")\n\n# Let's test it on a single sentence\ntest_sentence = \"This is a test sentence.\"\ntest_embedding = embeddings.embed_query(test_sentence)\n\nprint(f\"\\nEmbedding for the test sentence (first 5 dimensions): {test_embedding[:5]}\")\nprint(f\"Vector dimension: {len(test_embedding)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:21:50.468696Z","iopub.execute_input":"2025-11-07T09:21:50.468963Z","iopub.status.idle":"2025-11-07T09:22:24.626271Z","shell.execute_reply.started":"2025-11-07T09:21:50.468949Z","shell.execute_reply":"2025-11-07T09:22:24.625573Z"}},"outputs":[{"name":"stderr","text":"2025-11-07 09:22:03.485484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762507323.676223      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762507323.732158      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2fa37d35da440193668444268a2ef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc8e5e875c84a05bcb20fb8234c5b9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c478c33de6084fac93b882c39b2d1136"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdddb0167b3e4d8ca50aaddf22d618ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366d952154f5408898920df2df50dd10"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"294dbcb155bb4e1bbb9eea0e0a8d69e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b3503a800e46e9ba9d3f8a1d56a2c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823f96171c3d4c8f93b15ff4443882fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d26c0e611f465a95955081b5c4b903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bffd4cd651a34670a93e936703db27d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f6060a02264dfb881d0f17bda59cc6"}},"metadata":{}},{"name":"stdout","text":"Embedding model loaded successfully.\n\nEmbedding for the test sentence (first 5 dimensions): [0.08429646492004395, 0.05795372277498245, 0.004493352957069874, 0.1058211401104927, 0.007083415519446135]\nVector dimension: 384\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Code Demo 3: Storing and Searching in a Vector Database\n\n**Goal:** Set up a simple vector store (FAISS), add our documents, and perform a similarity search. (This part also remains the same!)","metadata":{}},{"cell_type":"code","source":"from langchain_community.vectorstores import FAISS\n\n# 4. Store the chunks and their embeddings in a vector store\n# FAISS (Facebook AI Similarity Search) is an efficient in-memory vector store.\nprint(\"Creating vector store from document chunks...\")\nvectorstore = FAISS.from_documents(chunks, embeddings)\nprint(\"Vector store created successfully!\")\n\n# Now, let's perform a similarity search\nquery = \"Who is Nefertiti?\"\nprint(f\"\\nQuery: '{query}'\")\n\n# Retrieve the top 3 most relevant chunks\nresults = vectorstore.similarity_search(query, k=3)\n\nprint(\"\\n--- Top 3 Relevant Chunks Found ---\")\nfor i, doc in enumerate(results):\n    print(f\"Result {i+1} (from page {doc.metadata.get('source', 'N/A')}):\\n{doc.page_content}\\n\")\n\n# Another example\nquery_2 = \"What was Nefertiti's reign like ?\"\nprint(f\"\\nQuery: '{query_2}'\")\nresults_2 = vectorstore.similarity_search(query_2, k=3)\n\nprint(\"\\n--- Top 3 Relevant Chunks Found ---\")\nfor i, doc in enumerate(results_2):\n    print(f\"Result {i+1} (from page {doc.metadata.get('source', 'N/A')}):\\n{doc.page_content}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:26:27.403642Z","iopub.execute_input":"2025-11-07T09:26:27.403920Z","iopub.status.idle":"2025-11-07T09:26:27.907879Z","shell.execute_reply.started":"2025-11-07T09:26:27.403905Z","shell.execute_reply":"2025-11-07T09:26:27.907126Z"}},"outputs":[{"name":"stdout","text":"Creating vector store from document chunks...\nVector store created successfully!\n\nQuery: 'Who is Nefertiti?'\n\n--- Top 3 Relevant Chunks Found ---\nResult 1 (from page https://en.wikipedia.org/wiki/Nefertiti):\n== Names and titles ==\nNefertiti had many titles, including:\n\nResult 2 (from page https://en.wikipedia.org/wiki/Nefertiti):\nThe exact dates when Nefertiti married Akhenaten and became the king's g\n\nResult 3 (from page https://en.wikipedia.org/wiki/Nefertiti):\nIt has also been proposed that Nefertiti was Akhenaten's full sister, though this is contradicted by her titles which do not include the title of \"King's Daughter\" or \"King's Sister,\" usually used to indicate a relative of a pharaoh. Another theory about her parentage that gained some support identified Nefertiti with the Mitanni princess Tadukhipa, partially based on Nefertiti's name (\"The Beautiful Woman has Come\") which has been interpreted by some scholars as signifying a foreign origin.\n\n\nQuery: 'What was Nefertiti's reign like ?'\n\n--- Top 3 Relevant Chunks Found ---\nResult 1 (from page https://en.wikipedia.org/wiki/Nefertiti):\nAfter her husband's death, some scholars believe that Nefertiti ruled briefly as the female pharaoh known by the throne name Neferneferuaten, and before the ascension of Tutankhamun, although this identification is a matter of ongoing debate. If Nefertiti did rule as pharaoh, her reign was marked by the fall of Amarna and the relocation of the capital back to the traditional city of Thebes.\n\nResult 2 (from page https://en.wikipedia.org/wiki/Nefertiti):\nThe exact dates when Nefertiti married Akhenaten and became the king's g\n\nResult 3 (from page https://en.wikipedia.org/wiki/Nefertiti):\nNefertiti () (c. 1370 – c. 1330 BC) was a queen of the 18th Dynasty of Ancient Egypt, the great royal wife of Pharaoh Akhenaten. Nefertiti and her husband were known for their radical overhaul of state religious policy, in which they promoted an exclusivist and possibly even monotheistic religion, Atenism, centered on the sun disc and its direct connection to the royal household. With her husband, she reigned at what was arguably the wealthiest period of ancient Egyptian history.\n\n","output_type":"stream"}],"execution_count":11}]}